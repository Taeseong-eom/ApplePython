{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1NMMus4fSPhNt_XaFfNJa4Q1t0Wnubw7y",
      "authorship_tag": "ABX9TyPm/OWchcnQCg/m7d/9Hpkh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Taeseong-eom/ApplePython/blob/main/dog_vs_cat_ipyn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jtb--SXahy23",
        "outputId": "3ab775ed-21cf-4892-e23d-29e12c05a451"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/kaggle\", line 5, in <module>\n",
            "    from kaggle.cli import main\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/kaggle/__init__.py\", line 23, in <module>\n",
            "    api.authenticate()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/kaggle/api/kaggle_api_extended.py\", line 403, in authenticate\n",
            "    raise IOError('Could not find {}. Make sure it\\'s located in'\n",
            "OSError: Could not find kaggle.json. Make sure it's located in /content/. Or use the environment method.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os. environ['KAGGLE_CONFIG_DIR']= '/content/'\n",
        "\n",
        "!kaggle competitions download -c dogs-vs-cats-redux-kernels-edition"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q dogs-vs-cats-redux-kernels-edition.zip -d ."
      ],
      "metadata": {
        "id": "F3XhDlMYnV7t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "133d26d5-6de3-419a-cf0c-6f61e6740693"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open dogs-vs-cats-redux-kernels-edition.zip, dogs-vs-cats-redux-kernels-edition.zip.zip or dogs-vs-cats-redux-kernels-edition.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q train.zip -d ."
      ],
      "metadata": {
        "id": "BJKsnluiqIR3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "310113dd-f719-42d3-ec53-c7b2f75a1ee0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open train.zip, train.zip.zip or train.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import shutil\n",
        "\n",
        "print (len(os.listdir('/content/train/'))) # 학습 이미지 개수가 2만5000개\n",
        "\n",
        "# 현재 train 폴더에는 개,고양이 사진이 같이 들어있다.\n",
        "# 개와 고양이 사진을 다른 폴더로 분류해보자.\n",
        "for i in os.listdir('/content/train/'):\n",
        "  if 'cat' in i: # 파일 이름에 'cat'이 들어있으면 '/content/dataset/cat/' + i 으로 복사해\n",
        "    shutil.copyfile('/content/train/' + i, '/content/dataset/cat/' + i)\n",
        "  elif 'dog' in i:# 파일 이름에 'dog'이 들어있으면 '/content/dataset/dog/' + i 으로 복사해\n",
        "    shutil.copyfile('/content/train/' + i, '/content/dataset/dog/' + i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "RLuUNthPqgzw",
        "outputId": "6fdffd3b-66e5-4000-bd24-c2159acd30f2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/train/'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-e39b09c683b4>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/train/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 학습 이미지 개수가 2만5000개\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# 현재 train 폴더에는 개,고양이 사진이 같이 들어있다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/train/'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 이미지를 숫자로 변환해야하는데 opencv라이브러리를 사용하거나\n",
        "# keras를 사용하면 변환할 수 있다.\n",
        "# keras가 간편하니 keras를 이용하겠다.\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory( # 아미지를 데이터셋 자료로 변환\n",
        "    '/content/dataset/', # 여기에 데이터들이 들어있어\n",
        "    image_size=(64,64), # 이미지 크기 조정\n",
        "    batch_size=64, # 이미지를 한번에 2만5천장을 넣지 않고 64장씩 넣겠다.\n",
        "    subset='training',# 데이터셋에서 20%를 valtidation으로 할당하겠다. 이름은 'training'으로 하겠다.\n",
        "    validation_split=0.2,\n",
        "    seed=1234,\n",
        ")\n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory( # 아미지를 데이터셋 자료로 변환\n",
        "    '/content/dataset/', # 여기에 데이터들이 들어있어\n",
        "    image_size=(64,64), # 이미지 크기 조정\n",
        "    batch_size=64,\n",
        "    subset='validation',\n",
        "    validation_split=0.2,\n",
        "    seed=1234,\n",
        ")\n",
        "print(train_ds)\n",
        "\n",
        "def 전처리함수(i, 정답):\n",
        "  i = tf.cast(i/255.0, tf.float32)\n",
        "  return i, 정답\n",
        "\n",
        "#숫자가 0 ~ 255 인걸 0 ~ 1로 압축을 하면 더 빨라진다.\n",
        "train_ds = train_ds.map(전처리함수) # train_ds의 데이터를 함수에 적용시켜라.\n",
        "val_ds = val_ds.map(전처리함수)\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "# # 데이터셋이 잘 변환되었는지 확인해보자\n",
        "# for i,정답 in train_ds.take(1): # 64개 데이터가 있는 데이터셋을 하나 가져옴.\n",
        "#   print(i)\n",
        "#   print(정답)\n",
        "#   plt.imshow(i[0].numpy().astype('uint8'))\n",
        "#   plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "o76aEqFluR0R",
        "outputId": "23ddfeb9-104f-4ce2-994b-1bca55508c44"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotFoundError",
          "evalue": "Could not find directory /content/dataset/",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-71edc4d4b39b>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# keras를 사용하면 변환할 수 있다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# keras가 간편하니 keras를 이용하겠다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m train_ds = tf.keras.preprocessing.image_dataset_from_directory( # 아미지를 데이터셋 자료로 변환\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;34m'/content/dataset/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# 여기에 데이터들이 들어있어\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mimage_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# 이미지 크기 조정\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/image_dataset.py\u001b[0m in \u001b[0;36mimage_dataset_from_directory\u001b[0;34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, crop_to_aspect_ratio, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m     image_paths, labels, class_names = dataset_utils.index_directory(\n\u001b[0m\u001b[1;32m    214\u001b[0m         \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/dataset_utils.py\u001b[0m in \u001b[0;36mindex_directory\u001b[0;34m(directory, labels, formats, class_names, shuffle, seed, follow_links)\u001b[0m\n\u001b[1;32m    540\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0msubdirs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mlist_directory_v2\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    766\u001b[0m   \"\"\"\n\u001b[1;32m    767\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m     raise errors.NotFoundError(\n\u001b[0m\u001b[1;32m    769\u001b[0m         \u001b[0mnode_def\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[0mop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFoundError\u001b[0m: Could not find directory /content/dataset/"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 이미지 증강 방법에는 2가지가 있다.\n",
        "# 1. 증강된 데이터 사본 저장\n",
        "# 2. 모델에 넣기 전에 이미지 증강 -> 간단함\n",
        "# 3. ImageDataGenerator를 사용 -> 연습해보자\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal',input_shape=(64,64,3)),\n",
        "    tf.keras.layers.experimental.preprocessing.RandomRotation(0.1),\n",
        "    tf.keras.layers.experimental.preprocessing.RandomZoom(0.1),\n",
        "\n",
        "    tf.keras.layers.Conv2D(32, (3,3), padding='same', activation='relu'),\n",
        "    tf.keras.layers.MaxPool2D((2,2)),\n",
        "    tf.keras.layers.Conv2D(64, (3,3), padding='same', activation='relu'),\n",
        "    tf.keras.layers.MaxPool2D((2,2)),\n",
        "    tf.keras.layers.Dropout(0.2), # Overfitting을 완화하기 위해 노트의 20%를 제거.\n",
        "    tf.keras.layers.Conv2D(128, (3,3), padding='same', activation='relu'), # convolution을 많이 하면 정확도가 좋아\n",
        "    tf.keras.layers.MaxPool2D((2,2)),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(1,activation='sigmoid')# 0~1로 값 출력\n",
        "])\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model.fit(train_ds,validation_data=val_ds,epochs=5) # 전처리를 안하면 학습속도, 정확도가 안좋다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 937
        },
        "id": "F21wtJgh10AS",
        "outputId": "6e502968-822d-4b85-9d72-21476724d890"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " random_flip_1 (RandomFlip)  (None, 64, 64, 3)         0         \n",
            "                                                                 \n",
            " random_rotation_1 (RandomR  (None, 64, 64, 3)         0         \n",
            " otation)                                                        \n",
            "                                                                 \n",
            " random_zoom (RandomZoom)    (None, 64, 64, 3)         0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 64, 64, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 32, 32, 32)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 32, 32, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 16, 16, 64)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 16, 16, 64)        0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPoolin  (None, 8, 8, 128)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 8192)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               1048704   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1142081 (4.36 MB)\n",
            "Trainable params: 1142081 (4.36 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_ds' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-b86abfd52348>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 전처리를 안하면 학습속도, 정확도가 안좋다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'train_ds' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4vFAVI9h-Clo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}