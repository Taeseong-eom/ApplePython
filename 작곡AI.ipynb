{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Taeseong-eom/ApplePython/blob/main/%EC%9E%91%EA%B3%A1AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlXmzxGVocOx",
        "outputId": "00bd357c-31c9-4880-94ed-132c957081cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[[0. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]], shape=(2, 25, 31), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/pianoabc.txt','r') as file:\n",
        "  text = file.read()\n",
        "# 악보를 문자로 바꿀 수 있을까?\n",
        "# abc notation 쓰면 된다.\n",
        "# txt 파일은 악보를 txt로 바꿔놨다.\n",
        "\n",
        "# 이제 문자를 숫자로 만들어서 학습시키자.\n",
        "# 문자를 숫자로 바꾸는 방법\n",
        "# 1. Bag of words(단어주머니) 만들어서 숫자로 변환\n",
        "utext = list(set(text))\n",
        "utext.sort()\n",
        "# print(utext)\n",
        "\n",
        "# 문자 -> 숫자 변환 딕셔너리\n",
        "text_to_num = {}\n",
        "num_to_text = {}\n",
        "for i, data in enumerate(utext): # enumerate 로 변수 2개 사용\n",
        "  text_to_num[data] = i\n",
        "for i, data in enumerate(utext):\n",
        "  num_to_text[i] = data\n",
        "\n",
        "숫자화text=[]\n",
        "for i in text:\n",
        "  숫자화text.append(text_to_num[i])\n",
        "# print(숫자화text)\n",
        "\n",
        "# 그럼 이제 숫자화text를 가지고 trainX,Y를 만들어보자.\n",
        "\n",
        "trainX, trainY = [], []\n",
        "\n",
        "for i in range(0, len(숫자화text)-25):\n",
        "    trainX.append(숫자화text[i:i+25])\n",
        "    trainY.append(숫자화text[i+25])\n",
        "\n",
        "import numpy as np\n",
        "# 원핫인코딩할때 유니크한 문자가 많으면 부담된다.\n",
        "# 그럴때 embedding layer를 사용한다.\n",
        "\n",
        "# 원핫인코딩을 하자.\n",
        "trainX = tf.one_hot(trainX, 31)\n",
        "trainY = tf.one_hot(trainY, 31)\n",
        "print(trainX[0:2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kIjk3ko3Hf1",
        "outputId": "952281f6-4036-453e-cfc9-30c66b4aa9d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "4563/4563 - 134s - loss: 1.6126 - accuracy: 0.4932 - 134s/epoch - 29ms/step\n",
            "Epoch 2/100\n",
            "4563/4563 - 132s - loss: 1.2982 - accuracy: 0.5744 - 132s/epoch - 29ms/step\n",
            "Epoch 3/100\n",
            "4563/4563 - 130s - loss: 1.2009 - accuracy: 0.6048 - 130s/epoch - 29ms/step\n",
            "Epoch 4/100\n",
            "4563/4563 - 130s - loss: 1.1418 - accuracy: 0.6247 - 130s/epoch - 29ms/step\n",
            "Epoch 5/100\n",
            "4563/4563 - 129s - loss: 1.1031 - accuracy: 0.6369 - 129s/epoch - 28ms/step\n",
            "Epoch 6/100\n",
            "4563/4563 - 131s - loss: 1.0752 - accuracy: 0.6464 - 131s/epoch - 29ms/step\n",
            "Epoch 7/100\n",
            "4563/4563 - 130s - loss: 1.0515 - accuracy: 0.6536 - 130s/epoch - 29ms/step\n",
            "Epoch 8/100\n",
            "4563/4563 - 130s - loss: 1.0320 - accuracy: 0.6605 - 130s/epoch - 28ms/step\n",
            "Epoch 9/100\n",
            "4563/4563 - 130s - loss: 1.0152 - accuracy: 0.6660 - 130s/epoch - 28ms/step\n",
            "Epoch 10/100\n",
            "4563/4563 - 130s - loss: 1.0001 - accuracy: 0.6708 - 130s/epoch - 28ms/step\n",
            "Epoch 11/100\n",
            "4563/4563 - 130s - loss: 0.9864 - accuracy: 0.6751 - 130s/epoch - 28ms/step\n",
            "Epoch 12/100\n",
            "4563/4563 - 130s - loss: 0.9744 - accuracy: 0.6792 - 130s/epoch - 28ms/step\n",
            "Epoch 13/100\n",
            "4563/4563 - 131s - loss: 0.9625 - accuracy: 0.6835 - 131s/epoch - 29ms/step\n",
            "Epoch 14/100\n",
            "4563/4563 - 128s - loss: 0.9522 - accuracy: 0.6870 - 128s/epoch - 28ms/step\n",
            "Epoch 15/100\n",
            "4563/4563 - 129s - loss: 0.9424 - accuracy: 0.6907 - 129s/epoch - 28ms/step\n",
            "Epoch 16/100\n",
            "4563/4563 - 130s - loss: 0.9329 - accuracy: 0.6931 - 130s/epoch - 28ms/step\n",
            "Epoch 17/100\n",
            "4563/4563 - 130s - loss: 0.9241 - accuracy: 0.6960 - 130s/epoch - 28ms/step\n",
            "Epoch 18/100\n",
            "4563/4563 - 130s - loss: 0.9161 - accuracy: 0.6988 - 130s/epoch - 28ms/step\n",
            "Epoch 19/100\n",
            "4563/4563 - 129s - loss: 0.9080 - accuracy: 0.7015 - 129s/epoch - 28ms/step\n",
            "Epoch 20/100\n",
            "4563/4563 - 132s - loss: 0.9006 - accuracy: 0.7049 - 132s/epoch - 29ms/step\n",
            "Epoch 21/100\n",
            "4563/4563 - 130s - loss: 0.8940 - accuracy: 0.7066 - 130s/epoch - 28ms/step\n",
            "Epoch 22/100\n",
            "4563/4563 - 130s - loss: 0.8868 - accuracy: 0.7087 - 130s/epoch - 28ms/step\n",
            "Epoch 23/100\n",
            "4563/4563 - 130s - loss: 0.8811 - accuracy: 0.7108 - 130s/epoch - 28ms/step\n",
            "Epoch 24/100\n",
            "4563/4563 - 129s - loss: 0.8754 - accuracy: 0.7129 - 129s/epoch - 28ms/step\n",
            "Epoch 25/100\n",
            "4563/4563 - 129s - loss: 0.8694 - accuracy: 0.7152 - 129s/epoch - 28ms/step\n",
            "Epoch 26/100\n",
            "4563/4563 - 129s - loss: 0.8642 - accuracy: 0.7162 - 129s/epoch - 28ms/step\n",
            "Epoch 27/100\n",
            "4563/4563 - 130s - loss: 0.8599 - accuracy: 0.7183 - 130s/epoch - 28ms/step\n",
            "Epoch 28/100\n"
          ]
        }
      ],
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.LSTM(100, input_shape=(25,31)), # 노드수\n",
        "    tf.keras.layers.Dense(31, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy']) # 원핫인코딩이 되어 있어서 sparse를 안붙여도 됨.\n",
        "model.fit(trainX, trainY, batch_size=64, epochs=100, verbose=2) # batch_size=64 == 64번 할때마다 w 값 업데이트 // verbose=2 출력 값이 많으면 다운될 수 있어서 출력 많이 하지 말라 명령.\n",
        "model.save('/content/drive/MyDrive/Colab Notebooks')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Qx6eUTG_rMsjyX0393tgteHV5qI0HpPv",
      "authorship_tag": "ABX9TyMuDvnSuiqqP+8szaHKmWB5",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}